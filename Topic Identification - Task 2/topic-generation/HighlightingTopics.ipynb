{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dca1d3e-a78e-4beb-b07d-b36fadeeb036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art Ids: ['70301542', '19294535', '89442655', '90301555', '69329387', '79442689', '39442672', '09442678', '90510865', '89278259', '29278304', '80401292', '29442615', '69442680', '50454507', '19189030', '59442685', '49442676', '70538846', '19442654', '29481701', '40501892', '49189203', '80487813', '19442692', '59189472', '79278250', '59294557', '99017445', '79442694']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=\"csv/GroundTruthV2.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "# Extract columns into separate arrays\n",
    "art_ids = df.iloc[:, 0].tolist()  # First column\n",
    "topics = df.iloc[:, 1].tolist()  # Second column\n",
    "\n",
    "art_ids = [str(number).zfill(8) for number in art_ids] #Add 0 padding for articleIDs\n",
    "\n",
    "# Print the arrays to verify\n",
    "print(\"Art Ids:\", art_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "707157cd-69e4-4b71-9804-79dfe4d9fec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "all_reviews = []\n",
    "\n",
    "for art_id in art_ids:\n",
    "    query = f\"\"\"SELECT concat(title,'. ',text) as text FROM `ingka-feed-student-dev.RR.RatingsReviews` AS rr\n",
    "                INNER JOIN `ingka-feed-student-dev.RR.product_categories` AS pc \n",
    "                ON rr.art_id = SPLIT(pc.global_id, ',')[SAFE_OFFSET(1)] \n",
    "                WHERE country_code = 'us' and PRODUCT_AREA = 'Open storage' and art_id = '{art_id}'\n",
    "                ORDER BY inserted_on DESC \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    article_reviews = []\n",
    "\n",
    "    for review in query_job:\n",
    "         article_reviews.append(review.text)\n",
    "\n",
    "    all_reviews.append(article_reviews)\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97395126-af4f-4e8f-8921-68da5b2abf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=\"csv/0.6TopP_5ShotLlama3+Keyphrases_Topics.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "# Extract columns into separate arrays\n",
    "all_topics = df.iloc[:, 1].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e140582-3aed-4254-8305-2bb5a33a9af9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from sentence_transformers import SentenceTransformer, util\\nimport numpy as np\\nimport torch\\n\\n# Load a sentence transformer model\\nmodel = SentenceTransformer(\\'Embedding-model/mpnetv2\\')\\n\\ndef find_reviews_with_topic(reviews, topic_phrase, max_reviews=4):\\n    topic_reviews = []\\n\\n    # Embed the topic phrase\\n    topic_embedding = model.encode(topic_phrase, convert_to_tensor=True)\\n\\n    # Iterate through each review\\n    for review in reviews:\\n        # Break the review into sentences and encode\\n        sentences = review.split(\\'. \\')\\n        sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\\n        similarities = util.pytorch_cos_sim(topic_embedding, sentence_embeddings)\\n\\n        # Check if any sentence has a similarity above the threshold\\n        max_similarity, max_index = torch.max(similarities, dim=1)  # Find max similarity and its index\\n        if max_similarity > 0.8:  # Using a threshold for similarity\\n            highlighted_sentence = sentences[max_index.item()].replace(topic_phrase, f\"**{topic_phrase}**\")\\n            topic_reviews.append(review.replace(sentences[max_index.item()], f\"**{highlighted_sentence}**\"))\\n        \\n        if len(topic_reviews) >= 4:\\n            break\\n\\n    return topic_reviews\\n\\ndef save_to_csv(data, filename):\\n    with open(filename, \\'w\\', newline=\\'\\', encoding=\\'utf-8\\') as file:\\n        writer = csv.writer(file)\\n        writer.writerow([\\'articleID\\', \\'topic\\', \\'reviews\\'])\\n        for item in data:\\n            writer.writerow(item)\\n\\ncsv_data = []\\ni=0\\nfor reviews, art_id in zip(all_reviews, art_ids):\\n    topics = all_topics[i].split(\\', \\')\\n    i+=1\\n    for topic in topics:\\n        matched_reviews = find_reviews_with_topic(reviews, topic)\\n        csv_data.append([art_id, topic, \\'| \\'.join(matched_reviews)]) '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load a sentence transformer model\n",
    "model = SentenceTransformer('Embedding-model/mpnetv2')\n",
    "\n",
    "def find_reviews_with_topic(reviews, topic_phrase, max_reviews=4):\n",
    "    topic_reviews = []\n",
    "\n",
    "    # Embed the topic phrase\n",
    "    topic_embedding = model.encode(topic_phrase, convert_to_tensor=True)\n",
    "\n",
    "    # Iterate through each review\n",
    "    for review in reviews:\n",
    "        # Break the review into sentences and encode\n",
    "        sentences = review.split('. ')\n",
    "        sentence_embeddings = model.encode(sentences, convert_to_tensor=True)\n",
    "        similarities = util.pytorch_cos_sim(topic_embedding, sentence_embeddings)\n",
    "\n",
    "        # Check if any sentence has a similarity above the threshold\n",
    "        max_similarity, max_index = torch.max(similarities, dim=1)  # Find max similarity and its index\n",
    "        if max_similarity > 0.8:  # Using a threshold for similarity\n",
    "            highlighted_sentence = sentences[max_index.item()].replace(topic_phrase, f\"**{topic_phrase}**\")\n",
    "            topic_reviews.append(review.replace(sentences[max_index.item()], f\"**{highlighted_sentence}**\"))\n",
    "        \n",
    "        if len(topic_reviews) >= 4:\n",
    "            break\n",
    "\n",
    "    return topic_reviews\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['articleID', 'topic', 'reviews'])\n",
    "        for item in data:\n",
    "            writer.writerow(item)\n",
    "\n",
    "csv_data = []\n",
    "i=0\n",
    "for reviews, art_id in zip(all_reviews, art_ids):\n",
    "    topics = all_topics[i].split(', ')\n",
    "    i+=1\n",
    "    for topic in topics:\n",
    "        matched_reviews = find_reviews_with_topic(reviews, topic)\n",
    "        csv_data.append([art_id, topic, '| '.join(matched_reviews)]) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03a2689e-86b3-4770-b307-d41eb58d0db9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import csv\\n\\n# Save to CSV\\nsave_to_csv(csv_data, \\'csv/topic_reviews08.csv\\')\\nprint(\"Data has been saved to \\'topic_reviews08.csv\\'.\") '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import csv\n",
    "\n",
    "# Save to CSV\n",
    "save_to_csv(csv_data, 'csv/topic_reviews08.csv')\n",
    "print(\"Data has been saved to 'topic_reviews08.csv'.\") \"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff835bc3-7d2b-40f4-b1ab-786d3caab68e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70301542\n",
      "19294535\n",
      "89442655\n",
      "90301555\n",
      "69329387\n",
      "79442689\n",
      "39442672\n",
      "09442678\n",
      "90510865\n",
      "89278259\n",
      "29278304\n",
      "80401292\n",
      "29442615\n",
      "69442680\n",
      "50454507\n",
      "19189030\n",
      "59442685\n",
      "49442676\n",
      "70538846\n",
      "19442654\n",
      "29481701\n",
      "40501892\n",
      "49189203\n",
      "80487813\n",
      "19442692\n",
      "59189472\n",
      "79278250\n",
      "59294557\n",
      "99017445\n",
      "79442694\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "\n",
    "# Load a sentence transformer model\n",
    "model = SentenceTransformer('Embedding-model/mpnetv2')\n",
    "\n",
    "# Check if a GPU is available and use it\n",
    "device ='cpu'\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "def find_reviews_with_topic(reviews, topic_phrase, max_reviews=4):\n",
    "    topic_reviews = []\n",
    "\n",
    "    # Embed the topic phrase\n",
    "    topic_embedding = model.encode(topic_phrase, convert_to_tensor=True).to(device)\n",
    "    # Iterate through each review\n",
    "    for review in reviews:\n",
    "        # Break the review into sentences\n",
    "        sentences = review.split('. ')\n",
    "        highlighted_review = review\n",
    "        found = False\n",
    "\n",
    "        for sentence in sentences:\n",
    "            \n",
    "            words = sentence.split()\n",
    "            \n",
    "            if not words:\n",
    "                continue\n",
    "\n",
    "            word_embeddings = model.encode(words, convert_to_tensor=True).to(device)\n",
    "            if word_embeddings.shape[0] == 0:\n",
    "                continue\n",
    "\n",
    "            similarities = util.pytorch_cos_sim(topic_embedding, word_embeddings)\n",
    "            max_similarity, max_index = torch.max(similarities, dim=1)\n",
    "            if max_similarity > 0.80:\n",
    "                index = max_index.item()\n",
    "                highlighted_word = words[index]\n",
    "                start_index = index\n",
    "                end_index = index\n",
    "\n",
    "                # Check if the highlighted word contains any special characters\n",
    "                contains_special_char = any(char in highlighted_word for char in [',', ';', '.'])\n",
    "\n",
    "                # Expand to the left\n",
    "                left_count = 0\n",
    "                while start_index > 0 and not any(char in words[start_index - 1] for char in [',', ';', '.', '!', '?']) and left_count < 3:\n",
    "                    start_index -= 1\n",
    "                    left_count += 1\n",
    "\n",
    "                # Expand to the right only if no special characters in the highlighted word\n",
    "                if not contains_special_char:\n",
    "                    right_count = 0\n",
    "                    while end_index < len(words) - 1 and words[end_index + 1] not in [',', ';', '.','!','?'] and right_count < 3:\n",
    "                        end_index += 1\n",
    "                        right_count += 1\n",
    "\n",
    "                # Form the highlighted phrase and update the sentence\n",
    "                highlighted_phrase = ' '.join(words[start_index:end_index + 1])\n",
    "                highlighted_sentence = sentence.replace(highlighted_phrase, f\"**{highlighted_phrase}**\")\n",
    "                highlighted_review = highlighted_review.replace(sentence, highlighted_sentence)\n",
    "                found = True\n",
    "\n",
    "\n",
    "        if found:\n",
    "            topic_reviews.append(highlighted_review)\n",
    "\n",
    "        if len(topic_reviews) >= max_reviews:\n",
    "            break\n",
    "\n",
    "    return topic_reviews\n",
    "\n",
    "\n",
    "def save_to_csv(data, filename):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['ArticleID', 'Topic', 'Reviews'])\n",
    "        for item in data:\n",
    "            writer.writerow(item)\n",
    "\n",
    "csv_data = []\n",
    "i = 0\n",
    "for reviews, art_id in zip(all_reviews, art_ids):\n",
    "    print(art_id)\n",
    "    topics = all_topics[i].split(', ')\n",
    "    i += 1\n",
    "    for topic in topics:\n",
    "        matched_reviews = find_reviews_with_topic(reviews, topic)\n",
    "        csv_data.append([art_id, topic, '| '.join(matched_reviews)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "776334b1-f677-46eb-ad2f-611c1eceffe9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been saved to csv/final_topic_reviews_word080.csv.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'csv/final_topic_reviews_word080.csv'\n",
    "\n",
    "# Save to CSV\n",
    "save_to_csv(csv_data, filename)\n",
    "print(f\"Data has been saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0a5483-c73f-4b83-ab92-926ff1a04ac1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
