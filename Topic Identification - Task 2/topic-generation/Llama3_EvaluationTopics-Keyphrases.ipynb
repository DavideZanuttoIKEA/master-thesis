{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3b1f61-bb89-42f5-a913-9f11b72d7a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art Ids: ['70301542', '19294535', '89442655', '90301555', '69329387', '79442689', '39442672', '09442678', '90510865', '89278259', '29278304', '80401292', '29442615', '69442680', '50454507', '19189030', '59442685', '49442676', '70538846', '19442654', '29481701', '40501892', '49189203', '80487813', '19442692', '59189472', '79278250', '59294557', '99017445', '79442694']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=\"csv/GroundTruthV2.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "# Extract columns into separate arrays\n",
    "art_ids = df.iloc[:, 0].tolist()  # First column\n",
    "topics = df.iloc[:, 1].tolist()  # Second column\n",
    "\n",
    "art_ids = [str(number).zfill(8) for number in art_ids] #Add 0 padding for articleIDs\n",
    "\n",
    "# Print the arrays to verify\n",
    "print(\"Art Ids:\", art_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "878be44b-b1ee-4ed2-abde-aa971b070735",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Nice and sturdy, Great storage space, easily assemble, loose construction,'], [], [], ['several sizes, works well, sturdy, great storage space, great value, easy to assemble, looks good, Arrived damaged,'], [\"Frustrating item, latch doesn't work, easy to clean underneath, permanent room feature, Architectural Digest look, crooked drawers,\"], ['looks good, optional drawers, would recommend Kallax products, easy assembly,'], [], ['looks wonderful, many options,'], [], ['great storage, Unable to purchase,'], ['Easy to transport, Kallax series, Installing doors and drawers, Durable, Excellent storage solution, Looks great, Great product, affordable, easy assembly,'], ['great use of space, well-built, Handy for craft supplies, for kallax shelves, holds wine bottles, Easy to assemble,'], ['comes together nicely, addition of legs,'], ['Built an entire wall, decent appearance, heavy-duty c-clamps, clear directions for base installation, several different colors, Works great for storage, Old school stereo console, perfect grey and light wood mix,'], ['Easy to assemble, works well, Decent rolling shelf, Oak appearance, Product was marked incorrectly,'], [], ['easy to assemble, well made, black base stand, Book shelf, sometimes out of stock, damaged side, Kallax for record albums,'], ['good value, functional, added inserts, easy assembly, can raise up on legs, storage below,'], ['easy assembly, Well designed, Good price, below windows, can hold large TV and soundbar,'], ['(No additional characteristics or sentiments given in the sentence), Great shelve,'], ['Functional, easy installation, killer line, wall anchors,'], ['well made, Looks great, Kallax with elegant legs, Simple stand, provides height, Affordable addition, Easy to build, makes bookcase more functional, Not fit old Kallax shelf,'], ['holds a lot, incompatible with glass window, tricky assembly, plastic attachment grooves,'], ['Ample shelf space, easy assembly,'], ['Worked perfectly, Easy to assemble, storage closet,'], [], ['Great for organizing, issue with doors,'], ['Thoughtful design, Great price,'], ['Great product, Excellent storage, provides shelving for small items, sturdy, love them all, Television stand, no clear sentiment expressed in sentence., looks great, easy assembly,'], ['provides storage, easy assembly,']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('csv/core_keyphrases_hierarchical.csv')  \n",
    "\n",
    "core_keyphrases = []\n",
    "for art_id in art_ids:\n",
    "    keyphrases = data[data['art_id'] == int(art_id)]['core_keyphrases'].tolist()\n",
    "    core_keyphrases.append(keyphrases)\n",
    "\n",
    "print(core_keyphrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "888d34f4-914a-4f12-9811-74725e6c104a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Art Ids: ['80508504', '49398678', '09294526', '10538849', '29278262']\n",
      "Topics: ['Quality, Price, Availability, Practicality, Style, Versatilite, Ease of Assembly, Screw', 'Quality, Ease of Assembly, Delivery', 'Quality, Size, Sturdiness, Space, Height, Ease of Assembly', 'Quality, Ease of Assembly, Instructions, Appearance, Material, Price, Design, Sturdiness', 'Quality, Ease of Assembly, Appearance, Versatility, Limitations, Value, Functionality, Sturdiness']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path=\"5Shot_Examples.csv\"\n",
    "\n",
    "df = pd.read_csv(file_path, sep=\";\")\n",
    "\n",
    "# Extract columns into separate arrays\n",
    "art_ids_5shot = df.iloc[:, 0].tolist()  # First column\n",
    "topics_5shot = df.iloc[:, 1].tolist()  # Second column\n",
    "\n",
    "art_ids_5shot = [str(number).zfill(8) for number in art_ids_5shot] #Add 0 padding for articleIDs\n",
    "\n",
    "# Print the arrays to verify\n",
    "print(\"Art Ids:\", art_ids_5shot)\n",
    "print(\"Topics:\", topics_5shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac0bce4c-019c-40da-ace4-fc0d8353d1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Good product, Kallax shelf unit, easy installation, limited availability, new price increase, stripped screws, white interior, wish for right/left options,'], ['easy to assemble, Exceptional pieces of storage, good experience,'], [\"Perfect storage solution, didn't always line up right,\"], ['nice material, include wall anchor kit, Clear installation instructions, pleasing shelf unit, carpenter friend found it challenging,'], ['Sturdy, Organization, Happy about product, Easy to find, junk doors, Easy assembly, good looking, slightly different,']]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('csv/core_keyphrases_hierarchical.csv')  \n",
    "\n",
    "core_keyphrases_5shot = []\n",
    "for art_id in art_ids_5shot:\n",
    "    keyphrases = data[data['art_id'] == int(art_id)]['core_keyphrases'].tolist()\n",
    "    core_keyphrases_5shot.append(keyphrases)\n",
    "\n",
    "print(core_keyphrases_5shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77f1936d-a3bd-45ce-a266-f31f1ad93ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "reviews_5shot = []\n",
    "\n",
    "for art_id in art_ids_5shot:\n",
    "    query = f\"\"\" SELECT concat(title,'. ',text) as text FROM `ingka-feed-student-dev.RR.RatingsReviews` AS rr\n",
    "                INNER JOIN `ingka-feed-student-dev.RR.product_categories` AS pc \n",
    "                ON rr.art_id = SPLIT(pc.global_id, ',')[SAFE_OFFSET(1)] \n",
    "                WHERE country_code = 'us' and PRODUCT_AREA = 'Open storage' and art_id = '{art_id}'\n",
    "                ORDER BY inserted_on DESC \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    article_reviews = []\n",
    "\n",
    "    for review in query_job:\n",
    "         article_reviews.append(review.text)\n",
    "\n",
    "    reviews_5shot.append(article_reviews)\n",
    "len(reviews_5shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79e49cb-9bf2-4d9a-9c8a-77b91fade4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a362769e56495881e9082ecc906f00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Some parameters are on the meta device device because they were offloaded to the cpu.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "access_token = \"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\"\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    token=access_token,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b96a990-21f1-43d7-809c-c6ee62d20b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th file completed in 70.40102338790894 seconds.\n",
      "20th file completed in 168.12382817268372 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "answers = []\n",
    "\n",
    "for core in core_keyphrases:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot who only answer with a comma-separated list of the requested topics and no additional text\"},\n",
    "        {\"role\": \"user\", \"content\": f'Analyze these product-related keyphrases and generate up to 8 broad, non-redundant topics that best categorize them. Topics should be single words where possible, or short phrases if necessary. Focus on product qualities, characteristics, or aspects of user experience (e.g., quality, ease of assembly, functionality). Avoid specific object names. Present topics as nouns or noun phrases (e.g., \"durability\" not \"durable\"). Consider both positive and negative aspects mentioned. Aim to create categories that could encompass multiple related keyphrases. Provide only a comma-separated list of topics. Keyphrases: \"{core_keyphrases_5shot[0]}\".'},        \n",
    "        {\"role\": \"assistant\", \"content\":f'{topics_5shot[0]}'},\n",
    "        {\"role\": \"user\", \"content\": f'Analyze these product-related keyphrases and generate up to 8 broad, non-redundant topics that best categorize them. Topics should be single words where possible, or short phrases if necessary. Focus on product qualities, characteristics, or aspects of user experience (e.g., quality, ease of assembly, functionality). Avoid specific object names. Present topics as nouns or noun phrases (e.g., \"durability\" not \"durable\"). Consider both positive and negative aspects mentioned. Aim to create categories that could encompass multiple related keyphrases. Provide only a comma-separated list of topics. Keyphrases: \"{core_keyphrases_5shot[1]}\".'},        \n",
    "        {\"role\": \"assistant\", \"content\":f'{topics_5shot[1]}'},\n",
    "        {\"role\": \"user\", \"content\": f'Analyze these product-related keyphrases and generate up to 8 broad, non-redundant topics that best categorize them. Topics should be single words where possible, or short phrases if necessary. Focus on product qualities, characteristics, or aspects of user experience (e.g., quality, ease of assembly, functionality). Avoid specific object names. Present topics as nouns or noun phrases (e.g., \"durability\" not \"durable\"). Consider both positive and negative aspects mentioned. Aim to create categories that could encompass multiple related keyphrases. Provide only a comma-separated list of topics. Keyphrases: \"{core_keyphrases_5shot[2]}\".'},        \n",
    "        {\"role\": \"assistant\", \"content\":f'{topics_5shot[2]}'},\n",
    "        {\"role\": \"user\", \"content\": f'Analyze these product-related keyphrases and generate up to 8 broad, non-redundant topics that best categorize them. Topics should be single words where possible, or short phrases if necessary. Focus on product qualities, characteristics, or aspects of user experience (e.g., quality, ease of assembly, functionality). Avoid specific object names. Present topics as nouns or noun phrases (e.g., \"durability\" not \"durable\"). Consider both positive and negative aspects mentioned. Aim to create categories that could encompass multiple related keyphrases. Provide only a comma-separated list of topics. Keyphrases: \"{core_keyphrases_5shot[3]}\".'},        \n",
    "        {\"role\": \"assistant\", \"content\":f'{topics_5shot[3]}'},\n",
    "        {\"role\": \"user\", \"content\": f'Analyze these product-related keyphrases and generate up to 8 broad, non-redundant topics that best categorize them. Topics should be single words where possible, or short phrases if necessary. Focus on product qualities, characteristics, or aspects of user experience (e.g., quality, ease of assembly, functionality). Avoid specific object names. Present topics as nouns or noun phrases (e.g., \"durability\" not \"durable\"). Consider both positive and negative aspects mentioned. Aim to create categories that could encompass multiple related keyphrases. Provide only a comma-separated list of topics. Keyphrases: \"{core_keyphrases_5shot[4]}\".'},        \n",
    "        {\"role\": \"assistant\", \"content\":f'{topics_5shot[4]}'},\n",
    "        {\"role\": \"user\", \"content\": f'Analyze these product-related keyphrases and generate up to 8 broad, non-redundant topics that best categorize them. Topics should be single words where possible, or short phrases if necessary. Focus on product qualities, characteristics, or aspects of user experience (e.g., quality, ease of assembly, functionality). Avoid specific object names. Present topics as nouns or noun phrases (e.g., \"durability\" not \"durable\"). Consider both positive and negative aspects mentioned. Aim to create categories that could encompass multiple related keyphrases. Provide only a comma-separated list of topics. Keyphrases: \"{core}\".'},        \n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=terminators,    \n",
    "    pad_token_id=pipeline.tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    "    )\n",
    "\n",
    "    answer=outputs[0][\"generated_text\"][len(prompt):]\n",
    "    answers.append(answer)\n",
    "    if (answers.index(answer) + 1) % 10 == 0: \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{answers.index(answer) + 1}th file completed in {elapsed_time} seconds.\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc32d85d-369e-4b01-8f94-6897b4a1a81c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv/GTv2/5ShotLlama3KeyphrasesHierarchical3.csv written with limited answer length!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = \"csv/GTv2/5ShotLlama3KeyphrasesHierarchical3.csv\"\n",
    "\n",
    "def process_answer(answer, max_words=8):\n",
    "    return ','.join(answer.split(',')[:max_words])\n",
    "\n",
    "def write_csv(filename, art_ids, answers, max_words=8):\n",
    "  \n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        # Creating a csv writer object with quoting set to quote all fields\n",
    "        csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "\n",
    "        # Writing the columns\n",
    "        csvwriter.writerow(['ArticleID', 'Topics'])\n",
    "\n",
    "        # Writing the data with processed answers\n",
    "        for article, answer in zip(art_ids, answers):\n",
    "            processed_answer = process_answer(answer, max_words)\n",
    "            csvwriter.writerow([article, processed_answer])\n",
    "\n",
    "write_csv(filename, art_ids, answers)\n",
    "print(f\"{filename} written with limited answer length!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95095fbb-9a5a-4b56-bb92-b359e9dcc557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
