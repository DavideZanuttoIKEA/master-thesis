{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f673940-0f6d-488d-96f6-deacef225af4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "ename": "SafetensorError",
     "evalue": "Error while deserializing header: MetadataIncompleteBuffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSafetensorError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize the model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEmbedding-model/gtebase\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m topics_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-evaluation/MetaEvaluation-Topics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     11\u001b[0m ground_truth_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-evaluation/MetaEvaluation-Topics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:286\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, prompts, default_prompt_name, similarity_fn_name, cache_folder, trust_remote_code, revision, local_files_only, token, use_auth_token, truncate_dim, model_kwargs, tokenizer_kwargs, config_kwargs, model_card_data)\u001b[0m\n\u001b[1;32m    277\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sentence_transformer_model(\n\u001b[1;32m    280\u001b[0m     model_name_or_path,\n\u001b[1;32m    281\u001b[0m     token,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    285\u001b[0m ):\n\u001b[0;32m--> 286\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_sbert_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    298\u001b[0m     modules \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_auto_model(\n\u001b[1;32m    299\u001b[0m         model_name_or_path,\n\u001b[1;32m    300\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    307\u001b[0m         config_kwargs\u001b[38;5;241m=\u001b[39mconfig_kwargs,\n\u001b[1;32m    308\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:1475\u001b[0m, in \u001b[0;36mSentenceTransformer._load_sbert_model\u001b[0;34m(self, model_name_or_path, token, cache_folder, revision, trust_remote_code, local_files_only, model_kwargs, tokenizer_kwargs, config_kwargs)\u001b[0m\n\u001b[1;32m   1472\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config_kwargs:\n\u001b[1;32m   1473\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_args\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupdate(config_kwargs)\n\u001b[0;32m-> 1475\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1476\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1477\u001b[0m     \u001b[38;5;66;03m# Normalize does not require any files to be loaded\u001b[39;00m\n\u001b[1;32m   1478\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module_class \u001b[38;5;241m==\u001b[39m Normalize:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:53\u001b[0m, in \u001b[0;36mTransformer.__init__\u001b[0;34m(self, model_name_or_path, max_seq_length, model_args, tokenizer_args, config_args, cache_dir, do_lower_case, tokenizer_name_or_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m     config_args \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     52\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_args, cache_dir\u001b[38;5;241m=\u001b[39mcache_dir)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seq_length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m tokenizer_args:\n\u001b[1;32m     56\u001b[0m     tokenizer_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_max_length\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m max_seq_length\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:84\u001b[0m, in \u001b[0;36mTransformer._load_model\u001b[0;34m(self, model_name_or_path, config, cache_dir, **model_args)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_mt5_model(model_name_or_path, config, cache_dir, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_args)\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/modeling_utils.py:3531\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3511\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m get_checkpoint_shard_files(\n\u001b[1;32m   3512\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m   3513\u001b[0m         resolved_archive_file,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3523\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m   3524\u001b[0m     )\n\u001b[1;32m   3526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   3527\u001b[0m     is_safetensors_available()\n\u001b[1;32m   3528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[1;32m   3529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   3530\u001b[0m ):\n\u001b[0;32m-> 3531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msafe_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m   3532\u001b[0m         metadata \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mmetadata()\n\u001b[1;32m   3534\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m metadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mSafetensorError\u001b[0m: Error while deserializing header: MetadataIncompleteBuffer"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import distance\n",
    "import json\n",
    "\n",
    "# Initialize the model\n",
    "model = SentenceTransformer(\"Embedding-model/gtebase\")\n",
    "\n",
    "topics_df = pd.read_csv('meta-evaluation/MetaEvaluation-Topics.csv', sep=';', usecols=[0,1])\n",
    "ground_truth_df = pd.read_csv('meta-evaluation/MetaEvaluation-Topics.csv', sep=';', usecols=[0,2])\n",
    "\n",
    "\n",
    "# Parse the GroundTruth field into a dictionary\n",
    "def parse_ground_truth(ground_truth_str):\n",
    "    return {k.strip(): int(v.strip()) for k, v in (item.split(':') for item in ground_truth_str.split(','))}\n",
    "\n",
    "ground_truth_df['Set2'] = ground_truth_df['Set2'].apply(parse_ground_truth)\n",
    "\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def calculate_similarity(embeddings1, embeddings2):\n",
    "    return util.pytorch_cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "\n",
    "# Calculate diversity based on embeddings\n",
    "def calculate_diversity(embeddings):\n",
    "    n = len(embeddings)\n",
    "    if n < 2:\n",
    "        return 0  # No diversity score if less than 2 topics\n",
    "    distances = []\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            dist = 1 - util.pytorch_cos_sim(embeddings[i], embeddings[j])\n",
    "            adjusted_dist = dist / 2  # Adjusting the range to 0 to 1\n",
    "            distances.append(adjusted_dist)\n",
    "    return sum(distances) / len(distances) if distances else 0\n",
    "\n",
    "\n",
    "def compute_metrics(article_id, topics, ground_truth):\n",
    "    \n",
    "    ground_truth_topics = ground_truth[article_id]\n",
    "    ground_truth_sorted = sorted(ground_truth_topics.items(), key=lambda item: item[1], reverse=True)\n",
    "     \n",
    "    N = min(8, len(ground_truth_sorted))\n",
    "    top_n_ground_truth = dict(ground_truth_sorted[:N])\n",
    "    correct_topics = set(topics).intersection(top_n_ground_truth.keys())\n",
    "    \n",
    "    ## EXACT-MATCHING\n",
    "    \n",
    "    # Exact-Match Precision\n",
    "    ExactMatch_precision = len(correct_topics) / len(topics) if topics else 0\n",
    "    \n",
    "    # Exact-Match Weighted Recall\n",
    "    correct_weights = sum(top_n_ground_truth[topic] for topic in correct_topics)\n",
    "    total_weight_top_n = sum(top_n_ground_truth.values())\n",
    "    ExactMatch_weighted_recall = correct_weights / total_weight_top_n if total_weight_top_n else 0\n",
    "    \n",
    "    # Exact-Match F1 Score\n",
    "    ExactMatch_f1_score = 2 * (ExactMatch_precision * ExactMatch_weighted_recall) / (ExactMatch_precision + ExactMatch_weighted_recall) if ExactMatch_precision + ExactMatch_weighted_recall != 0 else 0\n",
    "    \n",
    "    ## EMBEDDING-BASED\n",
    "    \n",
    "    # Embeddings\n",
    "    topic_embeddings = model.encode(list(topics))\n",
    "    gt_topic_embeddings = model.encode(list(top_n_ground_truth.keys()))\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = calculate_similarity(topic_embeddings, gt_topic_embeddings)\n",
    "    \n",
    "    # Compute Precision\n",
    "    max_similarity_per_identified = similarity_matrix.max(dim=1).values\n",
    "    Embeddings_precision = max_similarity_per_identified.mean().item()\n",
    "    \n",
    "    # Compute Weighted Recall\n",
    "    max_similarity_per_gt = similarity_matrix.max(dim=0).values\n",
    "    weighted_similarity_scores = max_similarity_per_gt * torch.tensor(list(top_n_ground_truth.values()))\n",
    "    Embeddings_weighted_recall = weighted_similarity_scores.sum().item() / sum(top_n_ground_truth.values())\n",
    "    \n",
    "    # Compute F1 Score\n",
    "    if Embeddings_precision + Embeddings_weighted_recall == 0:\n",
    "        Embeddings_f1_score = 0\n",
    "    else:\n",
    "        Embeddings_f1_score = 2 * (Embeddings_precision * Embeddings_weighted_recall) / (Embeddings_precision + Embeddings_weighted_recall)\n",
    "    \n",
    "    ## TOPIC DIVERSITY\n",
    "    \n",
    "    diversity_score = calculate_diversity(topic_embeddings)\n",
    "    \n",
    "    return ExactMatch_precision, ExactMatch_weighted_recall, ExactMatch_f1_score, Embeddings_precision, Embeddings_weighted_recall, Embeddings_f1_score, diversity_score\n",
    "\n",
    "# Apply metrics to all articles\n",
    "ExactMatch_results = []\n",
    "Embeddings_results = []\n",
    "Diversity_results = []\n",
    "\n",
    "for _, row in topics_df.iterrows():\n",
    "    article_id = row['ArticleID']\n",
    "    topics = row['Set1'].split(', ')\n",
    "    \n",
    "    ExactMatch_precision, ExactMatch_weighted_recall, ExactMatch_f1_score, Embeddings_precision, Embeddings_weighted_recall, Embeddings_f1_score, diversity = compute_metrics(article_id, topics, ground_truth_df.set_index('ArticleID')['Set2'])\n",
    "    \n",
    "    ExactMatch_results.append({'ArticleID': article_id, 'ExactMatch_precision': ExactMatch_precision, 'ExactMatch_weighted_recall': ExactMatch_weighted_recall, 'ExactMatch_f1_score': ExactMatch_f1_score})\n",
    "    Embeddings_results.append({'ArticleID': article_id, 'Embeddings_precision': Embeddings_precision, 'Embeddings_weighted_recall': Embeddings_weighted_recall, 'Embeddings_f1_score': Embeddings_f1_score})\n",
    "    Diversity_results.append({'ArticleID': article_id, 'Diversity': diversity})\n",
    "\n",
    "    \n",
    "ExactMatch_results_df = pd.DataFrame(ExactMatch_results)\n",
    "Embeddings_results_df = pd.DataFrame(Embeddings_results)\n",
    "Diversity_results_df = pd.DataFrame(Diversity_results)\n",
    "\n",
    "ExactMatch_final_precision = ExactMatch_results_df['ExactMatch_precision'].mean()\n",
    "ExactMatch_final_weighted_recall = ExactMatch_results_df['ExactMatch_weighted_recall'].mean()\n",
    "ExactMatch_final_f1_score = ExactMatch_results_df['ExactMatch_f1_score'].mean()\n",
    "\n",
    "Embeddings_final_precision = Embeddings_results_df['Embeddings_precision'].mean()\n",
    "Embeddings_final_weighted_recall = Embeddings_results_df['Embeddings_weighted_recall'].mean()\n",
    "Embeddings_final_f1_score = Embeddings_results_df['Embeddings_f1_score'].mean()\n",
    "\n",
    "final_diversity = Diversity_results_df['Diversity'].mean()\n",
    "\n",
    "ExactMatch_final_scores = {'Final Precision': ExactMatch_final_precision, 'Final Weighted Recall': ExactMatch_final_weighted_recall, 'Final F1 Score': ExactMatch_final_f1_score}\n",
    "Embeddings_scores = {'Final Precision': Embeddings_final_precision, 'Final Weighted Recall': Embeddings_final_weighted_recall, 'Final F1 Score': Embeddings_final_f1_score}\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "csv_df = pd.read_csv('meta-evaluation/MetaEvaluation-Topics.csv', sep=';')\n",
    "\n",
    "# Assume you have a common column in both DataFrames to merge on, e.g., 'ID'\n",
    "# Adjust 'ID' to your actual column name used for matching\n",
    "merged_df = pd.merge(csv_df, ExactMatch_results_df[['ArticleID', 'ExactMatch_f1_score']], on='ArticleID', how='left')\n",
    "merged_df2 = pd.merge(merged_df, Embeddings_results_df[['ArticleID', 'Embeddings_f1_score']], on='ArticleID', how='left')\n",
    "\n",
    "print(merged_df2)\n",
    "# Save the updated DataFrame back to CSV\n",
    "#merged_df.to_csv('meta-evaluation/MetaEvaluation-Topics.csv', sep=';')\n",
    "\n",
    "print(\"Finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcaf70d-214f-49c4-8ecd-3418fee0006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import pearsonr, spearmanr, kendalltau\n",
    "\n",
    "# Function to calculate correlations and p-values\n",
    "def calculate_correlations(df, col1, col2):\n",
    "    # Pearson correlation\n",
    "    pearson_corr, pearson_p = pearsonr(df[col1], df[col2])\n",
    "    print(f\"Pearson correlation between {col1} and {col2}: {pearson_corr:.3f}, p-value: {pearson_p:.3g}\")\n",
    "    \n",
    "    # Spearman correlation\n",
    "    spearman_corr, spearman_p = spearmanr(df[col1], df[col2])\n",
    "    print(f\"Spearman correlation between {col1} and {col2}: {spearman_corr:.3f}, p-value: {spearman_p:.3g}\")\n",
    "    \n",
    "    # Kendall correlation\n",
    "    kendall_corr, kendall_p = kendalltau(df[col1], df[col2])\n",
    "    print(f\"Kendall correlation between {col1} and {col2}: {kendall_corr:.3f}, p-value: {kendall_p:.3g}\")\n",
    "    \n",
    "# Calculate correlations between score1 and score2\n",
    "calculate_correlations(merged_df2, 'Score', 'ExactMatch_f1_score')\n",
    "\n",
    "# Calculate correlations between score1 and score3\n",
    "calculate_correlations(merged_df2, 'Score', 'Embeddings_f1_score')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3923d55-199c-40dd-a90c-8a97d19422ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40c4342-f7ce-40d0-8f8b-23949363bb5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
