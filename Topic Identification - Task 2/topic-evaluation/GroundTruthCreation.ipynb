{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "913e8dac-b1ab-4857-9e96-9aa2d1940d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['50339292', '30275861', '20423720', '40415601', '90324509']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from google.cloud import bigquery\n",
    "client = bigquery.Client()\n",
    "\n",
    "query = \"\"\" SELECT Distinct art_id as text FROM `ingka-feed-student-dev.RR.RatingsReviews` AS rr\n",
    "            INNER JOIN `ingka-feed-student-dev.RR.product_categories` AS pc \n",
    "            ON rr.art_id = SPLIT(pc.global_id, ',')[SAFE_OFFSET(1)] \n",
    "            WHERE country_code = 'us' and PRODUCT_AREA = 'Open storage' \"\"\"\n",
    "\n",
    "query_job = client.query(query)\n",
    "\n",
    "art_ids = []\n",
    "\n",
    "for art_id in query_job:\n",
    "     art_ids.append(art_id.text)\n",
    "\n",
    "print(len(art_ids))\n",
    "art_ids[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51910a0d-6d8d-481d-93ad-c5c24e7e09ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['19294535',\n",
       " '29294554',\n",
       " '90301555',\n",
       " '69329387',\n",
       " '79442689',\n",
       " '39442672',\n",
       " '09442678',\n",
       " '90510865',\n",
       " '89278259',\n",
       " '29278304',\n",
       " '80401292',\n",
       " '29442615',\n",
       " '69442680',\n",
       " '50454507',\n",
       " '19189030',\n",
       " '59442685',\n",
       " '49442676',\n",
       " '70538846',\n",
       " '19442654',\n",
       " '29481701',\n",
       " '40501892',\n",
       " '49189203',\n",
       " '80487813',\n",
       " '19442692',\n",
       " '59189472',\n",
       " '70301542',\n",
       " '79278250',\n",
       " '59294557',\n",
       " '99017445',\n",
       " '79442694']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "sample_size = min(30, len(art_ids))\n",
    "    \n",
    "ground_truth_ids = random.sample(art_ids, sample_size)\n",
    "\n",
    "ground_truth_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9ebff2-5ea3-430b-b09e-7945c9b8b436",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews = []\n",
    "\n",
    "for art_id in ground_truth_ids:\n",
    "    query = f\"\"\" SELECT concat(title,'. ',text) as text FROM `ingka-feed-student-dev.RR.RatingsReviews` AS rr\n",
    "                INNER JOIN `ingka-feed-student-dev.RR.product_categories` AS pc \n",
    "                ON rr.art_id = SPLIT(pc.global_id, ',')[SAFE_OFFSET(1)] \n",
    "                WHERE country_code = 'us' and PRODUCT_AREA = 'Open storage' and art_id = '{art_id}'\n",
    "                ORDER BY inserted_on DESC \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "\n",
    "    article_reviews = []\n",
    "\n",
    "    for review in query_job:\n",
    "         article_reviews.append(review.text)\n",
    "\n",
    "    all_reviews.append(article_reviews)\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebfcb499-ffae-4c7d-a9f1-e545495b911d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "\n",
    "def count_tokens_in_reviews(review_list):\n",
    "    total_tokens = 0\n",
    "    for review in review_list:\n",
    "        total_tokens += len(word_tokenize(review))\n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82e2ffbe-e8ef-4369-91d8-f7f58ec75a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_reviews_by_token_count(reviews, max_tokens):\n",
    "    above_limit = []\n",
    "    below_limit = []\n",
    "\n",
    "    for review_group in reviews:\n",
    "        token_count = count_tokens_in_reviews(review_group)\n",
    "        if token_count > max_tokens:\n",
    "            above_limit.append(review_group)\n",
    "        else:\n",
    "            below_limit.append(review_group)\n",
    "    \n",
    "    return below_limit, above_limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eedc1797-2a5a-4ae6-9c80-a5d216b5ce38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Products with <= 5000 tokens:\n",
      "29\n",
      "Products with > 5000 tokens:\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Applying the function\n",
    "below_limit, above_limit = categorize_reviews_by_token_count(all_reviews, max_tokens=5000)\n",
    "\n",
    "print(\"Products with <= 5000 tokens:\")\n",
    "print(len(below_limit))\n",
    "\n",
    "print(\"Products with > 5000 tokens:\")\n",
    "print(len(above_limit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84a2a83c-1320-4c71-aa94-8940b2a83cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sets of selected reviews: 1\n",
      "Number of reviews in the first set: 145\n",
      "Number of selected reviews in the first set: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure you have the NLTK tokenizers\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to count tokens\n",
    "def count_tokens(text):\n",
    "    return len(word_tokenize(text))\n",
    "\n",
    "above_limit_final = []\n",
    "selected_indices = []\n",
    "\n",
    "for reviews in above_limit:\n",
    "    start_reviews = [(i, review, count_tokens(review)) for i, review in enumerate(reviews)]\n",
    "    \n",
    "    random.shuffle(start_reviews)\n",
    "\n",
    "    selected_reviews = []\n",
    "    selected_indices_set = []\n",
    "    current_token_count = 0\n",
    "    for index, review, tokens in start_reviews:\n",
    "        if current_token_count + tokens > 5000:\n",
    "            break\n",
    "        selected_reviews.append(review)\n",
    "        current_token_count += tokens\n",
    "    \n",
    "    above_limit_final.append(selected_reviews)\n",
    "\n",
    "# Save the arrays of reviews to a JSON file\n",
    "with open('selected_reviews.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump({\"reviews\": above_limit_final}, file)\n",
    "\n",
    "# Output lengths of the final results\n",
    "print(\"Number of sets of selected reviews:\", len(above_limit_final))\n",
    "if above_limit_final:\n",
    "    print(\"Number of reviews in the first set:\", len(above_limit[0]))\n",
    "    print(\"Number of selected reviews in the first set:\", len(above_limit_final[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca7ffac-72dc-4b49-a522-7fc109ce3f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('selected_reviews.json', mode='w', encoding='utf-8') as file:\n",
    "    json.dump({\"reviews\": above_limit_final}, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53a52d70-119d-4e94-b571-5bbe8ab1ae2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(above_limit_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04766e2d-d6f1-47fc-afe5-d112c7484dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_reviews = above_limit_final + below_limit\n",
    "len(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b9664d3a-2669-4d45-bdde-a72759839247",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.31.0.dev0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.3.0)\n",
      "Requirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.23.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.4.3)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.66.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc2eb8afb5e41cc9b30596eb5083eaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "!pip install accelerate\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "access_token = \"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\"\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    token=access_token,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1504d1bd-3e40-437e-ab36-a0c61f33b547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th file completed in 30.23777461051941 seconds.\n",
      "20th file completed in 60.05793499946594 seconds.\n",
      "30th file completed in 87.76824188232422 seconds.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "answers = []\n",
    "\n",
    "for reviews in all_reviews:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful chatbot who only answer with a comma-separated list of the requested topics and no additional text\"},\n",
    "        {\"role\": \"user\", \"content\": 'From these reviews, generate maximum 15 non-redundant topics. Topics should be 1 word maximum, can be 2 or 3 if necessary. The topics should be about qualities and characteristics of the products (ex: quality, price, …) and not names of objects (like Books, shelves, …). The topics should be names (ex: not durable but durability). Answer only with a comma-separated list of the topics you identify. Reviews: \"Perfect Simple TV Stand. I actually went in to buy something different, but when I saw the possibility of changing the orientation of this bookshelf and combining with a stand (maybe not even necessary but I think it enhances the appearance), I bought this instead. It seems sturdy and durable, looks good, I’m happy with my purchase. I put a couple of Fossta bins in mine but there are many other options for the storage bays.\",\"Good quality. Good looking and quality\",\"TV Stand With Base. A Simple, clean and light weight unit when cocmbined with the metal base works well as a tv stand in my case a 55\" TV. I\\'ll probably get the black baskets which fit perfectly into this or craft something simple to cover the back. I was worried that the round bottom of the legs would not be secure into the sqare legs of the base but they seem to fit without movement. But be aware that the top is not secured in any way- meaning you can lift this kallax right up out of the base. Of course the weight of the tv will prevent that from occurring but ikea should have engineered a way to secure theKallax base to this Kallax unit.\",\"IKEA perfection. IKEA perfection\"\t,\"Sturdy, good quality and minimalist, Love it!. This media console is just what I needed! It simple, sturdy and minimalist. It was easy to built. Thanks to the visual instructions! Very good quality and it\\'s a minimalist piece. It adds to the TV room without interfering with the rest of the furniture but adding weigh to it, which was necessary. It is also useful because I store a blanket in one of the shelves, my sleepers in another one and tv stuff...\",\t\"Nice piece. I like it.\",\"Simple and elegant. Simple and elegant\".'},\n",
    "        {\"role\": \"assistant\", \"content\": \"Durability, Simplicity, Sturdiness, Quality, Weight, Minimalism, Appearance, Security, Ease of Assembly, Flexibility\"},\n",
    "        {\"role\": \"user\", \"content\": f'From these reviews, generate maximum 15 non-redundant topics. Topics should be 1 word maximum, can be 2 or 3 if necessary. The topics should be about qualities and characteristics of the products (ex: quality, price, …) and not names of objects (like Books, shelves, …). The topics should be names (ex: not durable but durability). Answer only with a comma-separated list of the topics you identify. Reviews: {reviews}.'},\n",
    "    ]\n",
    "\n",
    "    prompt = pipeline.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    terminators = [\n",
    "        pipeline.tokenizer.eos_token_id,\n",
    "        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "\n",
    "    outputs = pipeline(\n",
    "    prompt,\n",
    "    max_new_tokens=100,\n",
    "    eos_token_id=terminators,    \n",
    "    pad_token_id=pipeline.tokenizer.eos_token_id,\n",
    "    do_sample=True,\n",
    "    temperature=0.3,\n",
    "    top_p=0.9,\n",
    "    )\n",
    "\n",
    "    answer=outputs[0][\"generated_text\"][len(prompt):]\n",
    "    answers.append(answer)\n",
    "    if (answers.index(answer) + 1) % 10 == 0: \n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{answers.index(answer) + 1}th file completed in {elapsed_time} seconds.\")\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e3db9c3-611e-49e5-bf71-f1d93f85dd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file \"aGroundTruth_topics_llama3.csv\" has been written successfully.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "filename = 'aGroundTruth_topics_llama3.csv'\n",
    "\n",
    "# Writing to the csv file\n",
    "with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "    # Creating a csv writer object with quoting set to quote all fields\n",
    "    csvwriter = csv.writer(csvfile, delimiter=';')\n",
    "    \n",
    "    # Writing the columns\n",
    "    csvwriter.writerow(['ArticleID', 'Topics'])\n",
    "    \n",
    "    # Writing the data\n",
    "    for article, answer in zip(ground_truth_ids, answers):\n",
    "        csvwriter.writerow([article, answer])\n",
    "\n",
    "print(f'CSV file \"{filename}\" has been written successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cec09643-d932-485f-bef2-1f8767eb35b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def process_csv(input_file, output_file):\n",
    "    with open(input_file, mode='r', newline='', encoding='utf-8') as infile, \\\n",
    "         open(output_file, mode='w', newline='', encoding='utf-8') as outfile:\n",
    "        \n",
    "        reader = csv.reader(infile, delimiter=';')\n",
    "        writer = csv.writer(outfile, delimiter=';')\n",
    "        \n",
    "        for row in reader:\n",
    "            # Check if the row has at least two columns\n",
    "            if len(row) >= 2:\n",
    "                # Find the position of the first colon in the second column\n",
    "                colon_index = row[1].find(':')\n",
    "                # Remove everything before and including the colon, if it exists\n",
    "                if colon_index != -1:\n",
    "                    row[1] = row[1][colon_index + 1:].strip()\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Replace 'input.csv' and 'output.csv' with your actual file paths\n",
    "process_csv('aGroundTruth_topics_llama3.csv', 'GroundTruth_topics_llama3.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8d4c4c-d6e0-4693-83e2-1d14da72d7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7704896-0b58-443e-9976-a395d5dab4b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
