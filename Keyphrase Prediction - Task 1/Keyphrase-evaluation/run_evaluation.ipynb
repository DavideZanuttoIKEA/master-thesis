{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/davide.zanutto/miniconda3/envs/sample-thesis-project/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging CSV files...\n",
      "Cleaning merged data...\n",
      "Converting data to JSON...\n",
      "Calculating sentiment score...\n",
      "Processing row 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/rtm_6np11m34j4lvs1m7tpyr0000gp/T/ipykernel_7733/3824459026.py:28: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return data.applymap(remove_after_dash)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 5\n",
      "\n",
      "Sentiment Score: 0.82\n",
      "JSON data saved to keyphrase-generation/KPEval/model_outputs/sample/5shot_llama3/sample_hypotheses_linked.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "file = \"5shot_llama3\"\n",
    "csv_file_path = f'../results/{file}.csv'\n",
    "gt_file_path = 'KeyphrasesGroundTruth.csv'\n",
    "\n",
    "# Load the sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "\n",
    "def merge_and_process_csv(csv_data, gt_data):\n",
    "    csv_data = pd.read_csv(csv_file_path, usecols=[1], header=0, sep=\";\")\n",
    "    csv_data.columns = ['LLM']\n",
    "    gt_data = pd.read_csv(gt_file_path, usecols=[1], header=0, sep=\";\")\n",
    "    gt_data.columns = ['Ground truth']\n",
    "    merged_data = pd.concat([gt_data, csv_data], axis=1)\n",
    "    return merged_data\n",
    "\n",
    "def clean_data(data):\n",
    "    def remove_after_dash(text):\n",
    "        if isinstance(text, str):\n",
    "            return ', '.join(part.split(' - ')[0] for part in text.split(', '))\n",
    "        return text\n",
    "    return data.applymap(remove_after_dash)\n",
    "\n",
    "def data_to_json(data):\n",
    "    return [{\"source\": \"a\", \"target\": row['Ground truth'], \"predictions\": row['LLM']} for _, row in data.iterrows()]\n",
    "\n",
    "def save_json_file(data, file_name):\n",
    "    # Create directory if it doesn't exist\n",
    "    directory = f'model_outputs/sample/{file}'\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    \n",
    "    # Full path for the JSON file\n",
    "    file_path = f'{directory}/{file_name}'\n",
    "    \n",
    "    # Save JSON data to file, one object per line\n",
    "    with open(file_path, 'w') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f)\n",
    "            f.write('\\n')\n",
    "    \n",
    "    print(f\"JSON data saved to {file_path}\")\n",
    "def parse_phrases(sentiments_str):\n",
    "    return [phrase.strip() for phrase in sentiments_str.split(',')] if isinstance(sentiments_str, str) else []\n",
    "\n",
    "def compute_similarity(phrase1, phrase2):\n",
    "    embeddings1 = model.encode([phrase1])\n",
    "    embeddings2 = model.encode([phrase2])\n",
    "    return cosine_similarity(embeddings1, embeddings2)[0][0]\n",
    "\n",
    "def compare_sentiments(sent1, sent2):\n",
    "    return sent1.strip().lower().rstrip('.') == sent2.strip().lower().rstrip('.')\n",
    "\n",
    "def calculate_sentiment_score(data):\n",
    "    similarity_threshold = 0.7\n",
    "    total_score = 0\n",
    "    total_pairs = 0\n",
    "    for index, row in data.iterrows():\n",
    "        if index % 5 == 0:\n",
    "            print(f\"Processing row {index}\")\n",
    "        try:\n",
    "            ground_truth_phrases = parse_phrases(row['Ground truth'])\n",
    "            llm_phrases = parse_phrases(row['LLM'])\n",
    "            for gt_phrase in ground_truth_phrases:\n",
    "                if ' - ' not in gt_phrase:\n",
    "                    continue\n",
    "                gt_text, gt_sentiment = gt_phrase.rsplit(' - ', 1)\n",
    "                best_match = max(\n",
    "                    (phrase for phrase in llm_phrases if ' - ' in phrase),\n",
    "                    key=lambda phrase: compute_similarity(gt_text, phrase.rsplit(' - ', 1)[0]),\n",
    "                    default=None\n",
    "                )\n",
    "                if best_match:\n",
    "                    llm_text, llm_sentiment = best_match.rsplit(' - ', 1)\n",
    "                    if compute_similarity(gt_text, llm_text) >= similarity_threshold and compare_sentiments(gt_sentiment, llm_sentiment):\n",
    "                        total_score += 1\n",
    "                total_pairs += 1\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {index}: {e}\")\n",
    "    return total_score / total_pairs if total_pairs > 0 else 0\n",
    "\n",
    "# Main execution\n",
    "print(\"Merging CSV files...\")\n",
    "merged_data = merge_and_process_csv(csv_file_path, gt_file_path)\n",
    "\n",
    "print(\"Cleaning merged data...\")\n",
    "cleaned_data = clean_data(merged_data)\n",
    "\n",
    "print(\"Converting data to JSON...\")\n",
    "json_data = data_to_json(cleaned_data)\n",
    "\n",
    "print(\"Calculating sentiment score...\")\n",
    "sentiment_score = calculate_sentiment_score(merged_data)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\nSentiment Score: {sentiment_score:.2f}\")\n",
    "\n",
    "# Save JSON data to file\n",
    "save_json_file(json_data, 'sample_hypotheses_linked.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Cell 1\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Set the environment variable for PYTHONIOENCODING\n",
    "os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "\n",
    "# Set HOME_DIR and PYTHONPATH\n",
    "HOME_DIR = '.'\n",
    "os.environ['PYTHONPATH'] = os.environ.get('PYTHONPATH', '') + f':{HOME_DIR}'\n",
    "\n",
    "# Define other variables\n",
    "dataset = 'sample'\n",
    "model = '5shot_llama3'\n",
    "metrics = 'semantic_matching'\n",
    "OUTDIR = f'{HOME_DIR}/eval_results/{dataset}/{model}/'\n",
    "\n",
    "# Create the output directory\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading input files...\n",
      "================================================================================\n",
      "Calculating scores for the semantic_matching metric.\n",
      "{'semantic_p': 0.9644260823726654, 'semantic_r': 0.9644260942935944, 'semantic_f1': 0.9644260883331295}\n",
      "\n",
      "\n",
      "Preparing predictions:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Preparing predictions: 100%|██████████| 10/10 [00:00<00:00, 2997.64it/s]\n",
      "\n",
      "Preparing references:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Preparing references: 100%|██████████| 10/10 [00:00<00:00, 3269.14it/s]\n",
      "\n",
      "Preparing inputs:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Preparing inputs: 100%|██████████| 10/10 [00:00<00:00, 81284.96it/s]\n",
      "\n",
      "Evaluating...:   0%|          | 0/10 [00:00<?, ?it/s]\n",
      "Evaluating...:  10%|█         | 1/10 [00:03<00:33,  3.69s/it]\n",
      "Evaluating...:  20%|██        | 2/10 [00:03<00:12,  1.59s/it]\n",
      "Evaluating...:  30%|███       | 3/10 [00:03<00:06,  1.08it/s]\n",
      "Evaluating...:  40%|████      | 4/10 [00:04<00:03,  1.64it/s]\n",
      "Evaluating...:  50%|█████     | 5/10 [00:04<00:02,  2.33it/s]\n",
      "Evaluating...:  70%|███████   | 7/10 [00:04<00:00,  3.80it/s]\n",
      "Evaluating...:  80%|████████  | 8/10 [00:05<00:00,  2.34it/s]\n",
      "Evaluating...:  90%|█████████ | 9/10 [00:05<00:00,  2.81it/s]\n",
      "Evaluating...: 100%|██████████| 10/10 [00:06<00:00,  1.97it/s]\n",
      "Evaluating...: 100%|██████████| 10/10 [00:06<00:00,  1.58it/s]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Notebook Cell 2\n",
    "# Run the evaluation script using subprocess\n",
    "config_file = f'{HOME_DIR}/configs/sample_config_{dataset}.gin'\n",
    "jsonl_file = f'{HOME_DIR}/model_outputs/{dataset}/{model}/{dataset}_hypotheses_linked.json'\n",
    "log_file_prefix = OUTDIR\n",
    "\n",
    "command = [\n",
    "    'python', f'{HOME_DIR}/run_evaluation.py',\n",
    "    '--config-file', config_file,\n",
    "    '--jsonl-file', jsonl_file,\n",
    "    '--metrics', metrics,\n",
    "    '--log-file-prefix', log_file_prefix\n",
    "]\n",
    "\n",
    "# Execute the command\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "print(result.stdout)\n",
    "print(result.stderr)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample-thesis-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
